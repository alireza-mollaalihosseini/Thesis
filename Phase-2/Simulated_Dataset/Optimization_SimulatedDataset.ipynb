{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCNbNH-z10DR",
        "outputId": "6369874b-fcf8-4eed-bd9d-a05b6a2cea25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1636999168 bytes == 0x23f6000 @  0x7f59056281e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2\n",
            "tcmalloc: large alloc 2046255104 bytes == 0x63d20000 @  0x7f5905629615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\n",
            "tcmalloc: large alloc 1636999168 bytes == 0x23f6000 @  0x7f59056281e7 0x4d30a0 0x5dede2 0x6758aa 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4fe318 0x5da092 0x62042c 0x5d8d8c 0x561f80 0x4fd2db 0x4997c7 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m686.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.12.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp38-cp38-linux_x86_64.whl (22.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.12.0\n",
            "  Downloading torchtext-0.12.0-cp38-cp38-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11.0+cu113) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.12.0) (4.64.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0+cu113) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0+cu113) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0+cu113) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0+cu113) (2022.12.7)\n",
            "Installing collected packages: torch, torchvision, torchtext, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.1\n",
            "    Uninstalling torchtext-0.14.1:\n",
            "      Successfully uninstalled torchtext-0.14.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu116\n",
            "    Uninstalling torchaudio-0.13.1+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "Successfully installed torch-1.11.0+cu113 torchaudio-0.11.0+cu113 torchtext-0.12.0 torchvision-0.12.0+cu113\n"
          ]
        }
      ],
      "source": [
        "pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 torchtext==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9bxC-Iz011Jd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To install Falkon library\n",
        "!pip install git+https://github.com/falkonml/falkon.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33UKVXvA12sA",
        "outputId": "135cfd67-c765-4236-a54d-a8dfb2de87e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/falkonml/falkon.git\n",
            "  Cloning https://github.com/falkonml/falkon.git to /tmp/pip-req-build-wivm6ik9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/falkonml/falkon.git /tmp/pip-req-build-wivm6ik9\n",
            "  Resolved https://github.com/falkonml/falkon.git to commit e0f35851b00e181d6f2a5c52fc1aa160b1e7b0d6\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keopscore@ git+https://github.com/getkeops/keops@ad044a671fdc3c2790b0321f6b9f9b5aa3d220df#subdirectory=keopscore\n",
            "  Cloning https://github.com/getkeops/keops (to revision ad044a671fdc3c2790b0321f6b9f9b5aa3d220df) to /tmp/pip-install-pyfcw5jy/keopscore_3a264c14f8e442218372d9f8365d8162\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/getkeops/keops /tmp/pip-install-pyfcw5jy/keopscore_3a264c14f8e442218372d9f8365d8162\n",
            "  Running command git rev-parse -q --verify 'sha^ad044a671fdc3c2790b0321f6b9f9b5aa3d220df'\n",
            "  Running command git fetch -q https://github.com/getkeops/keops ad044a671fdc3c2790b0321f6b9f9b5aa3d220df\n",
            "  Running command git checkout -q ad044a671fdc3c2790b0321f6b9f9b5aa3d220df\n",
            "  Resolved https://github.com/getkeops/keops to commit ad044a671fdc3c2790b0321f6b9f9b5aa3d220df\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pykeops@ git+https://github.com/getkeops/keops@ad044a671fdc3c2790b0321f6b9f9b5aa3d220df#subdirectory=pykeops\n",
            "  Cloning https://github.com/getkeops/keops (to revision ad044a671fdc3c2790b0321f6b9f9b5aa3d220df) to /tmp/pip-install-pyfcw5jy/pykeops_7a1222f7243e44ffbc0bb601a7c88e39\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/getkeops/keops /tmp/pip-install-pyfcw5jy/pykeops_7a1222f7243e44ffbc0bb601a7c88e39\n",
            "  Running command git rev-parse -q --verify 'sha^ad044a671fdc3c2790b0321f6b9f9b5aa3d220df'\n",
            "  Running command git fetch -q https://github.com/getkeops/keops ad044a671fdc3c2790b0321f6b9f9b5aa3d220df\n",
            "  Running command git checkout -q ad044a671fdc3c2790b0321f6b9f9b5aa3d220df\n",
            "  Resolved https://github.com/getkeops/keops to commit ad044a671fdc3c2790b0321f6b9f9b5aa3d220df\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from falkon==0.7.5) (1.11.0+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from falkon==0.7.5) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from falkon==0.7.5) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from falkon==0.7.5) (1.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from falkon==0.7.5) (5.4.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->falkon==0.7.5) (4.4.0)\n",
            "Collecting pybind11\n",
            "  Downloading pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->falkon==0.7.5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->falkon==0.7.5) (1.2.0)\n",
            "Building wheels for collected packages: falkon, keopscore, pykeops\n",
            "  Building wheel for falkon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for falkon: filename=falkon-0.7.5-cp38-cp38-linux_x86_64.whl size=7555557 sha256=331c1a2d17ad73c04222fae020f928ac19c36ea6281e579efc560b2aee9336df\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yksos_uh/wheels/40/78/7d/48573e93aab7e41bfa1cd4677963d909f7fbe64930b706b70c\n",
            "  Building wheel for keopscore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keopscore: filename=keopscore-2.1-py3-none-any.whl size=147541 sha256=58788bbcac2e8e31b283e35383366932ca090b73d0263ed454c47502c546f67e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/ae/f5/30e7a89adb5ee516d1214872bd69c3a979a6b1a024aae64e6a\n",
            "  Building wheel for pykeops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykeops: filename=pykeops-2.1-py3-none-any.whl size=112058 sha256=36b72e1deba0c185a8f3751e663239eb6526d16fb80378d34d687b4e19509b1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/f3/5e/ef6d58318114b7c2855650e1418ec50d0fd03704243d935d95\n",
            "Successfully built falkon keopscore pykeops\n",
            "Installing collected packages: pybind11, keopscore, pykeops, falkon\n",
            "Successfully installed falkon-0.7.5 keopscore-2.1 pybind11-2.10.3 pykeops-2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import falkon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-4HTkOA14io",
        "outputId": "a9b40d4e-dbf8-4066-da18-95b471e16f70"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[KeOps] Warning : cuda was detected, but driver API could not be initialized. Switching to cpu only.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# we connect to the drive folder to work with the data stored\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slv4OB7t16yj",
        "outputId": "53a44662-5b87-4478-9ac9-c8382cefe166"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "s80dDEcw18aw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time"
      ],
      "metadata": {
        "id": "X4m3PniC1-j0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import os, time\n",
        "\n",
        "import torch\n",
        "\n",
        "from falkon import LogisticFalkon\n",
        "from falkon.kernels import GaussianKernel\n",
        "from falkon.options import FalkonOptions\n",
        "from falkon.gsc_losses import WeightedCrossEntropyLoss\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as font_manager\n",
        "\n",
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.stats import norm, chi2, rv_continuous, kstest\n",
        "\n",
        "\n",
        "# UTILS\n",
        "\n",
        "def candidate_sigma(data, perc=90):\n",
        "    # this function estimates the width of the gaussian kernel.\n",
        "    # use on a (small) sample of reference data (standardize first if necessary)\n",
        "    pairw = pdist(data)\n",
        "    return round(np.percentile(pairw,perc),1)\n",
        "\n",
        "'''\n",
        "def NP2_gen(size, seed):\n",
        "    # custom function to generate samples of non-resonant new physics events\n",
        "    if size>10000:\n",
        "        raise Warning('Sample size is grater than 1000: Generator will not approximate the tail well')\n",
        "    sample = np.array([])\n",
        "    #normalization factor                                                                                                                                    \n",
        "    np.random.seed(seed)\n",
        "    Norm = 256.*0.25*0.25*np.exp(-2)\n",
        "    while(len(sample)<size):\n",
        "        x = np.random.uniform(0,1) #assuming not to generate more than 10 000 events                                                                         \n",
        "        p = np.random.uniform(0, Norm)\n",
        "        if p<= 256.*x*x*np.exp(-8.*x):\n",
        "            sample = np.append(sample, x)\n",
        "    return sample\n",
        "'''\n",
        "\n",
        "class non_res(rv_continuous):\n",
        "\n",
        "    def _pdf(self, x):\n",
        "\n",
        "        return 256 * (x**2) * np.exp(- 8 * x)\n",
        "\n",
        "def nonres_sig(N_S, seed):\n",
        "    # this function can be used to generate non-resonant signal events.\n",
        "    \n",
        "    my_sig = non_res(momtype = 0, a=0, b=1, seed=seed)\n",
        "    \n",
        "    sig_sample = my_sig.rvs(size = N_S)\n",
        "    \n",
        "    return sig_sample\n",
        "\n",
        "\n",
        "def get_logflk_config(M,flk_sigma,lam,weight,iter=[100],seed=None,cpu=False):\n",
        "    # it returns logfalkon parameters\n",
        "    return {\n",
        "            'kernel' : GaussianKernel(sigma=flk_sigma),\n",
        "            'M' : M, #number of Nystrom centers,\n",
        "            'penalty_list' : lam, # list of regularization parameters,\n",
        "            'iter_list' : iter, #list of number of CG iterations,\n",
        "            'options' : FalkonOptions(cg_tolerance=np.sqrt(1e-7), keops_active='no', use_cpu=cpu, debug = False),\n",
        "            'seed' : seed, # (int or None), the model seed (used for Nystrom center selection) is manually set,\n",
        "            'loss' : WeightedCrossEntropyLoss(kernel=GaussianKernel(sigma=flk_sigma), neg_weight=weight),\n",
        "            }\n",
        "\n",
        "\n",
        "def compute_t(preds,Y,weight):\n",
        "    # it returns extended log likelihood ratio from predictions\n",
        "    diff = weight*np.sum(1 - np.exp(preds[Y==0]))\n",
        "    return 2 * (diff + np.sum(preds[Y==1]))\n",
        "\n",
        "def trainer(X,Y,flk_config):\n",
        "    # trainer for logfalkon model\n",
        "    Xtorch=torch.from_numpy(X)\n",
        "    Ytorch=torch.from_numpy(Y)\n",
        "    model = LogisticFalkon(**flk_config)\n",
        "    model.fit(Xtorch, Ytorch)\n",
        "    return model.predict(Xtorch).numpy()\n",
        "\n",
        "def standardize(X):\n",
        "    # standardize data as in HIGGS and SUSY\n",
        "    for j in range(X.shape[1]):\n",
        "        column = X[:, j]\n",
        "\n",
        "        mean = np.mean(column)\n",
        "        std = np.std(column)\n",
        "    \n",
        "        if np.min(column) < 0:\n",
        "            column = (column-mean)*1./ std\n",
        "        elif np.max(column) > 1.0:                                                                                                                                        \n",
        "            column = column *1./ mean\n",
        "    \n",
        "        X[:, j] = column\n",
        "    \n",
        "    return X\n",
        "\n",
        "def return_best_chi2dof(tobs):\n",
        "    \"\"\"\n",
        "    Returns the most fitting value for dof assuming tobs follows a chi2_dof distribution,\n",
        "    computed with a Kolmogorov-Smirnov test, removing NANs and negative values.\n",
        "    Parameters\n",
        "    ----------\n",
        "    tobs : np.ndarray\n",
        "        observations\n",
        "    Returns\n",
        "    -------\n",
        "        best : tuple\n",
        "            tuple with best dof and corresponding chi2 test result\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    dof_range = np.arange(np.nanmedian(tobs) - 10, np.nanmedian(tobs) + 10, 0.1)\n",
        "    \n",
        "    ks_tests = []\n",
        "    \n",
        "    for dof in dof_range:\n",
        "        \n",
        "        test = kstest(tobs, lambda x:chi2.cdf(x, df=dof))[0]\n",
        "        \n",
        "        ks_tests.append((dof, test))\n",
        "        \n",
        "    ks_tests = [test for test in ks_tests if test[1] != 'nan'] # remove nans\n",
        "    \n",
        "    ks_tests = [test for test in ks_tests if test[0] >= 0] # retain only positive dof\n",
        "        \n",
        "    best = min(ks_tests, key = lambda t: t[1]) # select best dof according to KS test result\n",
        "        \n",
        "    return best"
      ],
      "metadata": {
        "id": "nfJ80HdD2ANN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emp_zscore(t0,t1):\n",
        "    if max(t0) <= t1:\n",
        "        p_obs = 1 / len(t0)\n",
        "        Z_obs = round(norm.ppf(1 - p_obs),2)\n",
        "        return Z_obs\n",
        "    else:\n",
        "        p_obs = np.count_nonzero(t0 >= t1) / len(t0)\n",
        "        Z_obs = round(norm.ppf(1 - p_obs),2)\n",
        "        return Z_obs\n",
        "\n",
        "def chi2_zscore(t1, dof):\n",
        "    p = chi2.cdf(float('inf'),dof)-chi2.cdf(t1,dof)\n",
        "    return norm.ppf(1 - p)"
      ],
      "metadata": {
        "id": "w8XWO5gU2B0Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def err_bar(hist, n_samples):\n",
        "    \n",
        "    bins_counts = hist[0]\n",
        "    bins_limits = hist[1]\n",
        "    \n",
        "    x   = 0.5*(bins_limits[1:] + bins_limits[:-1])\n",
        "    \n",
        "    bins_width = 0.5*(bins_limits[1:] - bins_limits[:-1])\n",
        "    err = np.sqrt(np.array(bins_counts)/(n_samples*np.array(bins_width)))\n",
        "    \n",
        "    return x, err"
      ],
      "metadata": {
        "id": "oStJ4tyM2GYW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(data, label, name=None, dof=None, out_path=None, title=None,\n",
        "                 density=True, bins=10,\n",
        "                 c='mediumseagreen', e='darkgreen'):\n",
        "    \"\"\"\n",
        "    Plot reference vs new physics t distribution\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray or list\n",
        "        (N_toy,) array of observed test statistics\n",
        "    dof : int \n",
        "        degrees of freedom of the chi-squared distribution\n",
        "    name : string\n",
        "        filename for the saved figure\n",
        "    out_path : string, optional\n",
        "        output path where the figure will be saved. The default is ./fig.\n",
        "    title : string\n",
        "        title of the plot\n",
        "    density : boolean\n",
        "        True to normalize the histogram, false otherwise.\n",
        "    bins : int or string, optional\n",
        "        bins for the function plt.hist(). The default is 'fd'.\n",
        "    Returns\n",
        "    -------\n",
        "    plot\n",
        "    \"\"\"\n",
        "    \n",
        " \n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.style.use('classic')\n",
        "    \n",
        "\n",
        "    hist = plt.hist(data, bins = bins, color=c, edgecolor=e,\n",
        "                        density=density, label = str(label))\n",
        "    x_err, err = err_bar(hist, data.shape[0])\n",
        "    plt.errorbar(x_err, hist[0], yerr = err, color=e, marker='o', ms=6, ls='', lw=1,\n",
        "                 alpha=0.7)\n",
        "    \n",
        "\n",
        "    plt.ylim(bottom=0)\n",
        "    \n",
        "    # results data\n",
        "    md_t = round(np.median(data), 2)\n",
        "    if dof:\n",
        "        z_chi2 = round(chi2_zscore(np.median(data),dof=dof),2)\n",
        "    \n",
        "    if dof:\n",
        "        res = \"md t = {} \\nZ_chi2 = {}\".format(md_t,z_chi2)\n",
        "    else:\n",
        "        res = \"md t = {}\".format(md_t)\n",
        "\n",
        "    # plot chi2 and set xlim\n",
        "    if dof:\n",
        "        chi2_range = chi2.ppf(q=[0.00001,0.999], df=dof)\n",
        "        x = np.arange(chi2_range[0], chi2_range[1], .05)\n",
        "        chisq = chi2.pdf(x, df=dof)       \n",
        "        plt.plot(x, chisq, color='#d7191c', lw=2, label='$\\chi^2(${}$)$'.format(dof))\n",
        "        xlim = (min(chi2_range[0], min(data)-5), max(chi2_range[1], max(data)+5))\n",
        "        plt.xlim(chi2_range)\n",
        "    else:\n",
        "        xlim = (min(data)-5, max(data)+5)\n",
        "        plt.xlim(xlim)\n",
        "\n",
        "\n",
        "    if title:\n",
        "        plt.title(title, fontsize=20)\n",
        "    \n",
        "    plt.ylabel('P(t)', fontsize=20)\n",
        "    plt.xlabel('t', fontsize=20)\n",
        "    \n",
        "    # Axes ticks\n",
        "    ax = plt.gca()\n",
        "    \n",
        "    plt.legend(loc =\"upper right\", frameon=True, fontsize=18)\n",
        "    \n",
        "    ax.text(0.75, 0.65, res, color='black', fontsize=12,\n",
        "        bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=.5'),transform = ax.transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if out_path:\n",
        "        plt.savefig(out_path+\"/data_{}.pdf\".format(name), bbox_inches='tight')\n",
        "    \n",
        "    plt.show()\n",
        "        \n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "DJ-HRyeG2IMf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ref_data(ref, data, name=None, dof=None, out_path=None, title=None,\n",
        "                 density=True, bins=10,\n",
        "                 c_ref='#abd9e9', e_ref='#2c7bb6', c_sig='#fdae61', e_sig='#d7191c'):\n",
        "    \"\"\"\n",
        "    Plot reference vs new physics t distribution\n",
        "    Parameters\n",
        "    ----------\n",
        "    T_ref : np.ndarray or list\n",
        "        (N_toy,) array of observed test statistics in the reference hypothesis\n",
        "    T_sig : np.ndarray or list\n",
        "        (N_toy,) array of observed test statistics in the New Physics hypothesis\n",
        "    dof : int \n",
        "        degrees of freedom of the chi-squared distribution\n",
        "    name : string\n",
        "        filename for the saved figure\n",
        "    out_path : string, optional\n",
        "        output path where the figure will be saved. The default is ./fig.\n",
        "    title : string\n",
        "        title of the plot\n",
        "    density : boolean\n",
        "        True to normalize the histogram, false otherwise.\n",
        "    bins : int or string, optional\n",
        "        bins for the function plt.hist(). The default is 'fd'.\n",
        "    Returns\n",
        "    -------\n",
        "    plot\n",
        "    \"\"\"\n",
        "    \n",
        " \n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.style.use('classic')\n",
        "    #set uniform bins across all data points\n",
        "    bins = np.histogram(np.hstack((ref,data)), bins = bins)[1]\n",
        "    \n",
        "    # reference\n",
        "    hist_ref = plt.hist(ref, bins = bins, color=c_ref, edgecolor=e_ref,\n",
        "                        density=density, label = 'Reference')\n",
        "    x_err, err = err_bar(hist_ref, ref.shape[0])\n",
        "    plt.errorbar(x_err, hist_ref[0], yerr = err, color=e_ref, marker='o', ms=6, ls='', lw=1,\n",
        "                 alpha=0.7)\n",
        "    # data\n",
        "    hist_sig = plt.hist(data, bins = bins, color=c_sig, edgecolor=e_sig,\n",
        "                        alpha=0.7, density=density, label='Data')\n",
        "    x_err, err = err_bar(hist_sig, data.shape[0])\n",
        "    plt.errorbar(x_err, hist_sig[0], yerr = err, color=e_sig, marker='o', ms=6, ls='', lw=1,\n",
        "                 alpha=0.7)\n",
        "    \n",
        "\n",
        "    plt.ylim(bottom=0)\n",
        "    \n",
        "    # results data\n",
        "    md_tref = round(np.median(ref), 2)\n",
        "    md_tdata = round(np.median(data), 2)\n",
        "    max_zemp = emp_zscore(ref,np.max(ref))\n",
        "    zemp = emp_zscore(ref,np.median(data))\n",
        "    if dof:\n",
        "        z_chi2 = round(chi2_zscore(np.median(data),dof=dof),2)\n",
        "    \n",
        "    if dof:\n",
        "        res = \"md t_ref = {} \\nmd t_data = {} \\nmax Z_emp = {}  \\nZ_emp = {} \\nZ_chi2 = {}\".format(\n",
        "            md_tref,\n",
        "            md_tdata,\n",
        "            max_zemp,\n",
        "            zemp,\n",
        "            z_chi2\n",
        "        )\n",
        "    else:\n",
        "        res = \"md tref = {} \\nmd tdata = {} \\nmax Zemp = {} \\nZemp = {}\".format(\n",
        "            md_tref,\n",
        "            md_tdata,\n",
        "            max_zemp,\n",
        "            zemp\n",
        "        )\n",
        "\n",
        "    # plot chi2 and set xlim\n",
        "    if dof:\n",
        "        chi2_range = chi2.ppf(q=[0.00001,0.999], df=dof)\n",
        "        #r_len = chi2_range[1] - chi2_range[0]\n",
        "        x = np.arange(chi2_range[0], chi2_range[1], .05)\n",
        "        chisq = chi2.pdf(x, df=dof)       \n",
        "        plt.plot(x, chisq, color='#d7191c', lw=2, label='$\\chi^2(${}$)$'.format(dof))\n",
        "        xlim = (min(chi2_range[0], min(ref)-1), max(chi2_range[1], max(data)+1))\n",
        "        plt.xlim(xlim)\n",
        "    else:\n",
        "        xlim = (min(ref)-1, max(data)+1)\n",
        "        plt.xlim(xlim)\n",
        "\n",
        "\n",
        "    if title:\n",
        "        plt.title(title, fontsize=20)\n",
        "    \n",
        "    plt.ylabel('P(t)', fontsize=20)\n",
        "    plt.xlabel('t', fontsize=20)\n",
        "    \n",
        "    # Axes ticks\n",
        "    ax = plt.gca()\n",
        "    \n",
        "    plt.legend(loc =\"upper right\", frameon=True, fontsize=18)\n",
        "    \n",
        "    ax.text(0.75, 0.55, res, color='black', fontsize=12,\n",
        "        bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=.5'),transform = ax.transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if out_path:\n",
        "        plt.savefig(out_path+\"/refdata_{}.pdf\".format(name), bbox_inches='tight')\n",
        "        \n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "g8WpGGZF2KFE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_template_roc_curve(ax, title,fs,random=True):\n",
        "    \n",
        "    ax.set_title(title, fontsize=fs)\n",
        "    ax.set_xlim([-0.01, 1.01])\n",
        "    ax.set_ylim([-0.01, 1.01])\n",
        "    \n",
        "    ax.set_xlabel('False Positive Rate', fontsize=fs)\n",
        "    ax.set_ylabel('True Positive Rate', fontsize=fs)\n",
        "    \n",
        "    if random:\n",
        "        ax.plot([0, 1], [0, 1],'r--',label=\"AUC ROC Random = 0.5\")\n",
        "        \n",
        "def get_template_pr_curve(ax, title,fs, baseline=0.5):\n",
        "    ax.set_title(title, fontsize=fs)\n",
        "    ax.set_xlim([-0.01, 1.01])\n",
        "    ax.set_ylim([-0.01, 1.01])\n",
        "    \n",
        "    ax.set_xlabel('Recall (True Positive Rate)', fontsize=fs)\n",
        "    ax.set_ylabel('Precision', fontsize=fs)\n",
        "    \n",
        "    ax.plot([0, 1], [baseline, baseline],'r--',label='AP Random = 0.5')"
      ],
      "metadata": {
        "id": "x_qh6a-S2L6B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_plots(data_path, output_path, N_0, N0, NS, flk_config, df=10, savefig=True):\n",
        "  '''\n",
        "  plotting the AUC ROC, and PR curves\n",
        "  '''\n",
        "\n",
        "  weight = N0/N_0\n",
        "  # loading data\n",
        "  df = pd.read_csv(data_path)\n",
        "\n",
        "    \n",
        "  # dividing the dataset\n",
        "  fraud_df = df.loc[df['Class'] == 1]\n",
        "  non_fraud_df = df.loc[df['Class'] == 0]\n",
        "\n",
        "  dim = 2\n",
        "  rng = np.random.default_rng(123)\n",
        "\n",
        "  # Shuffling the data\n",
        "  df = df.sample(frac=1, random_state=123)\n",
        "  fraud_df = fraud_df.sample(frac=1,random_state=123)\n",
        "  non_fraud_df = non_fraud_df.sample(frac=1,random_state=123)\n",
        "\n",
        "  N0p = rng.poisson(lam=N0)\n",
        "  NSp = rng.poisson(lam=NS) # if data contains anomalies\n",
        "  N = N_0 + N0p + NSp\n",
        "\n",
        "  X = np.zeros(shape=(N,dim))\n",
        "  X[:N_0+N0p,:] = non_fraud_df[['V11','V14']].sample(n= N_0+N0p, replace=False, random_state=123) # ref and bkg\n",
        "  X[N_0+N0p:,:] = fraud_df[['V11','V14']].sample(n= NSp, replace=False, random_state=123) # signal\n",
        "  # initialize labes\n",
        "  Y = np.zeros(shape=(N,1))\n",
        "  Y[N_0:,:] = np.ones((N0p+NSp,1)) # flip data labels to one\n",
        "  X = np.array(X)\n",
        "\n",
        "  preds = trainer(X,Y,flk_config)\n",
        "\n",
        "  curve, ax = plt.subplots(2, 1, figsize=(15,10))\n",
        "\n",
        "  # AUC ROC curve\n",
        "\n",
        "  FPR_list, TPR_list, threshold = metrics.roc_curve(Y, preds)\n",
        "  ROC_AUC = metrics.auc(FPR_list, TPR_list)\n",
        "    \n",
        "  get_template_roc_curve(ax[0],title='Receiver Operating Characteristic Curve',fs=15)\n",
        "\n",
        "  ax[0].plot(FPR_list, TPR_list, 'b', label = 'AUC ROC = {0:0.3f}'.format(ROC_AUC))\n",
        "  ax[0].legend(loc = 'upper left',bbox_to_anchor=(1.05, 1))\n",
        "\n",
        "\n",
        "\n",
        "  # PR (Precision Recall) curve\n",
        "\n",
        "  get_template_pr_curve(ax[1], \"Precision Recall (PR) Curve\", fs=15, baseline = sum(np.array(Y))/len(np.array(Y)))\n",
        "\n",
        "  precision, recall, threshold = metrics.precision_recall_curve(Y, preds)\n",
        "  precision=precision[::-1]\n",
        "  recall=recall[::-1]\n",
        "\n",
        "  AP = metrics.average_precision_score(Y, preds)\n",
        "\n",
        "  ax[1].step(recall, precision, 'b', label = 'AP = {0:0.3f}'.format(AP))\n",
        "  ax[1].legend(loc = 'upper left',bbox_to_anchor=(1.05, 1))\n",
        "  plt.subplots_adjust(wspace=0.5, hspace=0.8)\n",
        "\n",
        "  if output_path:\n",
        "      plt.savefig(output_path+\"/metric_plots.pdf\", bbox_inches='tight')\n",
        "    \n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "qINJKMYb2OFq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reconstruction(df, data, weight_data, ref, weight_ref, t_obs, ref_preds,\n",
        "                        save=False, save_path='', file_name=''):\n",
        "    '''\n",
        "    Reconstruction of the data distribution learnt by the model.\n",
        "    \n",
        "    df:              (int) chi2 degrees of freedom\n",
        "    data:            (numpy array, shape (None, n_dimensions)) data training sample (label=1)\n",
        "    weight_data:     (numpy array, shape (None,)) weights of the data sample (default ones)\n",
        "    ref:             (numpy array, shape (None, n_dimensions)) reference training sample (label=0)\n",
        "    weight_ref:      (numpy array, shape (None,)) weights of the reference sample\n",
        "    tau_OBS:         (float) value of the tau term after training\n",
        "    output_tau_ref:  (numpy array, shape (None, 1)) tau prediction of the reference training sample after training\n",
        "    feature_labels:  (list of string) list of names of the training variables\n",
        "    bins_code:       (dict) dictionary of bins edge for each training variable (bins_code.keys()=feature_labels)\n",
        "    xlabel_code:     (dict) dictionary of xlabel for each training variable (xlabel.keys()=feature_labels)\n",
        "    ymax_code:       (dict) dictionary of maximum value for the y axis in the ratio panel for each training variable (ymax_code.keys()=feature_labels)\n",
        "    delta_OBS:       (float) value of the delta term after training (if not given, only tau reconstruction is plotted)\n",
        "    output_delta_ref:(numpy array, shape (None, 1)) delta prediction of the reference training sample after training (if not given, only tau reconstruction is plotted)\n",
        "    '''\n",
        "    # used to regularize empty reference bins\n",
        "    eps = 1e-10 \n",
        "\n",
        "    weight_ref = np.ones(len(ref))*weight_ref\n",
        "    weight_data = np.ones(len(data))*weight_data\n",
        "\n",
        "    # Dimention\n",
        "    dim = data.shape[-1]\n",
        "    \n",
        "    Zscore=norm.ppf(chi2.cdf(t_obs, df))\n",
        "\n",
        "    for i in range(dim):\n",
        "\n",
        "      bins = np.linspace(0,1.5,24)\n",
        "      plt.rcParams[\"font.family\"] = \"serif\"\n",
        "      plt.style.use('classic')\n",
        "      fig = plt.figure(figsize=(8, 8)) \n",
        "      fig.patch.set_facecolor('white')  \n",
        "      ax1= fig.add_axes([0.1, 0.43, 0.8, 0.5])        \n",
        "      hD = plt.hist(data[:, i],weights=weight_data, bins=bins, label='DATA', color='black', lw=1.5, histtype='step', zorder=2)\n",
        "      hR = plt.hist(ref[:, i], weights=weight_ref, color='#a6cee3', ec='#1f78b4', bins=bins, lw=1, label='REFERENCE', zorder=1)\n",
        "      hN = plt.hist(ref[:, i], weights=np.exp(ref_preds[:, 0])*weight_ref, histtype='step', bins=bins, lw=0)\n",
        "    \n",
        "      plt.errorbar(0.5*(bins[1:]+bins[:-1]), hD[0], yerr= np.sqrt(hD[0]), color='black', ls='', marker='o', ms=5, zorder=3)\n",
        "      plt.scatter(0.5*(bins[1:]+bins[:-1]),  hN[0], edgecolor='black', label='RECO', color='#b2df8a', lw=1, s=30, zorder=4)\n",
        "\n",
        "      font = font_manager.FontProperties(family='serif', size=16)\n",
        "      l    = plt.legend(fontsize=18, prop=font, ncol=2)\n",
        "      font = font_manager.FontProperties(family='serif', size=18) \n",
        "      title  = 't='+str(np.around(t_obs, 2))\n",
        "      \n",
        "      title += ', Z-score='+str(np.around(Zscore, 2))\n",
        "      l.set_title(title=title, prop=font)\n",
        "      plt.tick_params(axis='x', which='both',    labelbottom=False)\n",
        "      plt.yticks(fontsize=16, fontname='serif')\n",
        "      plt.xlim(0, 1.5)\n",
        "      plt.ylabel(\"events\", fontsize=22, fontname='serif')\n",
        "      plt.yscale('log')\n",
        "      ax2 = fig.add_axes([0.1, 0.1, 0.8, 0.3]) \n",
        "      x   = 0.5*(bins[1:]+bins[:-1])\n",
        "      plt.errorbar(x, hD[0]/(hR[0]+eps), yerr=np.sqrt(hD[0])/(hR[0]+eps), ls='', marker='o', label ='DATA/REF', color='black')\n",
        "      plt.plot(x, hN[0]/(hR[0]+eps), label ='RECO', color='#b2df8a', lw=3)\n",
        "\n",
        "      font = font_manager.FontProperties(family='serif', size=16)\n",
        "      plt.legend(fontsize=18, prop=font)\n",
        "      plt.xlabel('x', fontsize=22, fontname='serif')\n",
        "      plt.ylabel(\"ratio\", fontsize=22, fontname='serif')\n",
        "\n",
        "      plt.yticks(fontsize=16, fontname='serif')\n",
        "      plt.xticks(fontsize=16, fontname='serif')\n",
        "      plt.xlim(bins[0], bins[-1])\n",
        "      plt.ylim(0,10)\n",
        "      plt.grid()\n",
        "      if save:\n",
        "          os.makedirs(save_path, exist_ok=True)\n",
        "          fig.savefig(save_path + 'dim'+ '-'+ '{}'.format(i+1)+ file_name)\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "Tg1i-fvt2QHC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for 2-D with plot-reconstruction (with passing data)\n",
        "\n",
        "def run_toys(sig, data, fraud, genuine, output_path, N_0, N0, NS, flk_config, toys=np.arange(100), plots_freq=0, df=10, savefig=True):\n",
        "\n",
        "    '''\n",
        "    type of signal: \"NP0\", \"NP1\", \"NP2\", \"NP3\"\n",
        "    output_path: directory (inside ./runs/) where to save results\n",
        "    N_0: size of ref sample\n",
        "    N0: expected num of bkg events\n",
        "    NS: expected num of signal events\n",
        "    flk_config: dictionary of logfalkon parameters\n",
        "    toys: numpy array with seeds for toy generation\n",
        "    plots_freq: how often to plot inputs with learned reconstructions\n",
        "    df: degree of freedom of chi^2 for plots\n",
        "    '''\n",
        "\n",
        "    output_path = \"./runs/\" + output_path\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    #save config file (temporary solution)\n",
        "    with open(output_path+\"/flk_config.txt\",\"w\") as f:\n",
        "        f.write( str(flk_config) )\n",
        "\n",
        "    weight = N0/N_0\n",
        "    # passing data\n",
        "    samples = data\n",
        "\n",
        "    \n",
        "    # dividing the dataset\n",
        "    fraud_df = fraud\n",
        "    non_fraud_df = genuine\n",
        "\n",
        "    dim = 2\n",
        "\n",
        "    for i in toys:\n",
        "\n",
        "        st_time = time.time()\n",
        "\n",
        "        rng = np.random.default_rng(i)\n",
        "\n",
        "        # Shuffling the data\n",
        "        samples = samples.sample(frac=1, random_state=i)\n",
        "        fraud_df = fraud_df.sample(frac=1,random_state=i)\n",
        "        non_fraud_df = non_fraud_df.sample(frac=1,random_state=i)\n",
        "\n",
        "        N0p = rng.poisson(lam=N0)\n",
        "        if sig!=\"NP0\": NSp = rng.poisson(lam=NS) # if data contains anomalies\n",
        "        else: NSp = 0\n",
        "\n",
        "        N = N_0 + N0p + NSp\n",
        "\n",
        "        #print(\"[--] Toy {}: \".format(i))\n",
        "        # build training set\n",
        "        # initialize dataset\n",
        "        # fill with ref, bkg and data\n",
        "        if sig==\"NP0\":\n",
        "            X = non_fraud_df[['scaled_CUSTOMER_ID','scaled_TERMINAL_ID']].sample(n= N, replace=False, random_state=i) # both reference and data contain only bkg events (no NP component)\n",
        "        else:\n",
        "            X = np.zeros(shape=(N,dim))\n",
        "            X[:N_0+N0p,:] = non_fraud_df[['scaled_CUSTOMER_ID','scaled_TERMINAL_ID']].sample(n= N_0+N0p, replace=False, random_state=i) # ref and bkg\n",
        "            X[N_0+N0p:,:] = fraud_df[['scaled_CUSTOMER_ID','scaled_TERMINAL_ID']].sample(n= NSp, replace=False, random_state=i) # signal\n",
        "        # initialize labes\n",
        "        Y = np.zeros(shape=(N,1))\n",
        "        Y[N_0:,:] = np.ones((N0p+NSp,1)) # flip data labels to one\n",
        "        X = np.array(X)\n",
        "        \n",
        "        #print(\"[--] Reference shape:{}\".format(X[Y.flatten()==0].shape))\n",
        "        #print(\"[--] Data shape:{}\".format(X[Y.flatten()==1].shape))\n",
        "        \n",
        "        # in this 1D case, there is no need to standardize\n",
        "        #Xoriginal = X.copy()\n",
        "        #X = standardize(X)\n",
        "\n",
        "        # learn_t\n",
        "        flk_config['seed']=i # select different centers for different toys\n",
        "\n",
        "        preds = trainer(X,Y,flk_config)\n",
        "\n",
        "        t = compute_t(preds,Y,weight)\n",
        "\n",
        "        dt = round(time.time()-st_time,2)\n",
        "\n",
        "        #print(\"t = {}\\nTime = {} sec\\n\\t\".format(t,dt))\n",
        "\n",
        "        with open(output_path+\"t.txt\", 'a') as f:\n",
        "            f.write('{},{}\\n'.format(i,t))\n",
        "\n",
        "        #if plots_freq!=0 and t>0 and i in toys[::plots_freq]:\n",
        "        #    plot_reconstruction(data=X[Y.flatten()==1], weight_data=1, ref=X[Y.flatten()==0], weight_ref=weight, df=df, t_obs=t, ref_preds=preds[Y.flatten()==0],\n",
        "        #                save=savefig, save_path=output_path+'/plots/', file_name='sig_'+sig+'_NS{}_seed{}.pdf'.format(NS,i)\n",
        "        #            )"
      ],
      "metadata": {
        "id": "UGq6fYH_2S2x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(ref, data, dof=None):\n",
        "  md_tref = round(np.median(ref), 2)\n",
        "  md_tdata = round(np.median(data), 2)\n",
        "  max_zemp = emp_zscore(ref,np.max(ref))\n",
        "  zemp = emp_zscore(ref,np.median(data))\n",
        "  if dof:\n",
        "      z_chi2 = round(chi2_zscore(np.median(data),dof=dof),2)\n",
        "\n",
        "  return {\"md_tref\":md_tref,\n",
        "          \"md_tdata\":md_tdata,\n",
        "          \"max_zemp\":max_zemp,\n",
        "          \"zemp\":zemp,\n",
        "          \"dof\":dof,\n",
        "          \"z_chi2\":z_chi2}"
      ],
      "metadata": {
        "id": "Ug7Mo6yP2jLo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def GridSearch(parms):\n",
        "\n",
        "  res = []\n",
        "\n",
        "  # Loading the dataset\n",
        "  creds = pd.read_csv(\"/content/drive/MyDrive/SimulatedDataset/SimulatedDataset.csv\")\n",
        "  creds = creds[['CUSTOMER_ID','TERMINAL_ID','TX_FRAUD']]\n",
        "  \n",
        "  from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "  # RobustScaler is less prone to outliers.\n",
        "\n",
        "  std_scaler = StandardScaler()\n",
        "  rob_scaler = RobustScaler()\n",
        "\n",
        "  creds['scaled_CUSTOMER_ID'] = rob_scaler.fit_transform(creds['CUSTOMER_ID'].values.reshape(-1,1))\n",
        "  creds['scaled_TERMINAL_ID'] = rob_scaler.fit_transform(creds['TERMINAL_ID'].values.reshape(-1,1))\n",
        "\n",
        "  creds.drop(['CUSTOMER_ID','TERMINAL_ID'], axis=1, inplace=True)\n",
        "  scaled_CUSTOMER_ID = creds['scaled_CUSTOMER_ID']\n",
        "  scaled_TERMINAL_ID = creds['scaled_TERMINAL_ID']\n",
        "\n",
        "  creds.drop(['scaled_CUSTOMER_ID', 'scaled_TERMINAL_ID'], axis=1, inplace=True)\n",
        "  creds.insert(0, 'scaled_CUSTOMER_ID', scaled_CUSTOMER_ID)\n",
        "  creds.insert(1, 'scaled_TERMINAL_ID', scaled_TERMINAL_ID)\n",
        "\n",
        "  fraud_samples = creds.loc[creds['TX_FRAUD'] == 1]\n",
        "  genuine_samples = creds.loc[creds['TX_FRAUD'] == 0]\n",
        "\n",
        "  combinations = itertools.product(*(parms[Name] for Name in parms.keys()))\n",
        "  for row in list(combinations):\n",
        "\n",
        "    # M should be less than N_0 and N0\n",
        "    if row[2]<row[0] and row[2]<row[1]:\n",
        "      if row[0]>row[1]:\n",
        "        # parameters\n",
        "        flk_sigma = 3\n",
        "        lam = 1e-7\n",
        "        N_0, N0, M, NS = row[0], row[1], row[2], row[3]\n",
        "        weight = N0/N_0\n",
        "\n",
        "        flk_config = get_logflk_config(M,flk_sigma,[lam],weight=weight,iter=[100],seed=None,cpu=True)\n",
        "\n",
        "        # Running the codes on reference dataset \n",
        "        NS_temp = 0\n",
        "        run_toys(\"NP0\", creds, fraud_samples, genuine_samples, \"./reference/\", N_0, N0, NS_temp, flk_config, toys=np.arange(30), df= 22)\n",
        "\n",
        "        # Best DoF\n",
        "        best = return_best_chi2dof(np.loadtxt('/content/runs/reference/t.txt',delimiter=',')[:,1])\n",
        "\n",
        "        # Running the codes on all data\n",
        "        run_toys(\"NP1\", creds, fraud_samples, genuine_samples, \"./NP1/\", N_0, N0, NS, flk_config, toys=np.arange(30), df= best[0])\n",
        "\n",
        "        # Collecting the important results for comparison\n",
        "        results = get_results(np.loadtxt('/content/runs/reference/t.txt',delimiter=',')[:,1],np.loadtxt('/content/runs/NP1/t.txt',delimiter=',')[:,1],dof=best[0])\n",
        "\n",
        "        # adding the parameters info to the collected data for comparison\n",
        "        results.update({\"N_0\": N_0, \"N0\": N0, \"M\": M, \"NS\": NS})\n",
        "        # Updating the whole Results\n",
        "        res.append(results)\n",
        "\n",
        "  return res"
      ],
      "metadata": {
        "id": "z6UFlr9c2ldi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fkrvfXP3o0j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\"N_0\":[1000,20000,50000],\"N0\":[1000,5000,10000],\"M\":[800,1500],\"NS\":[100,500]}\n",
        "my_dict = GridSearch(parameters)\n",
        "res_df = pd.DataFrame(my_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-56uAtS32yE",
        "outputId": "d81b4c2b-052b-48b8-f4fd-9012c413f922"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n",
            "Iteration 0 - penalty 1.000000e-07 - sub-iterations 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "4P0pLPVG4v_r",
        "outputId": "50972ef6-f44b-4dc9-e613-538dcd050157"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    md_tref  md_tdata  max_zemp  zemp        dof  z_chi2    N_0     N0     M  \\\n",
              "0      9.78     19.17      1.83  1.83  10.279797    1.71  20000   1000   800   \n",
              "1      9.78    107.60      2.13  2.13  10.279797     inf  20000   1000   800   \n",
              "2     11.13     24.16      2.29  1.70  11.525841    2.16  20000   5000   800   \n",
              "3     11.40     42.04      2.39  2.39  12.103694    3.98  20000   5000   800   \n",
              "4     11.58     25.80      2.47  1.55  12.580420    2.17  20000   5000  1500   \n",
              "5     11.69     37.63      2.54  2.54  12.786955    3.44  20000   5000  1500   \n",
              "6     12.29     27.46      2.59  1.83  13.192731    2.27  20000  10000   800   \n",
              "7     12.52     30.77      2.64  2.13  13.615087    2.58  20000  10000   800   \n",
              "8     13.33     27.46      2.68  1.66  13.931871    2.14  20000  10000  1500   \n",
              "9     13.55     30.37      2.71  1.75  14.149312    2.44  20000  10000  1500   \n",
              "10    12.70     28.48      2.74  1.66  13.596791    2.32  50000   1000   800   \n",
              "11    12.23     31.05      2.77  2.01  13.230633    2.68  50000   1000   800   \n",
              "12    11.92     28.95      2.80  1.87  13.023077    2.47  50000   5000   800   \n",
              "13    11.70     31.26      2.82  2.07  12.804010    2.77  50000   5000   800   \n",
              "14    11.64     28.95      2.84  1.93  12.638322    2.54  50000   5000  1500   \n",
              "15    11.54     31.47      2.87  2.13  12.544567    2.84  50000   5000  1500   \n",
              "16    11.55     28.95      2.88  1.99  12.450808    2.57  50000  10000   800   \n",
              "17    11.66     30.30      2.90  2.01  12.462405    2.72  50000  10000   800   \n",
              "18    11.69     27.30      2.92  1.75  12.387421    2.39  50000  10000  1500   \n",
              "19    11.72     29.42      2.94  2.05  12.420991    2.63  50000  10000  1500   \n",
              "\n",
              "     NS  \n",
              "0   100  \n",
              "1   500  \n",
              "2   100  \n",
              "3   500  \n",
              "4   100  \n",
              "5   500  \n",
              "6   100  \n",
              "7   500  \n",
              "8   100  \n",
              "9   500  \n",
              "10  100  \n",
              "11  500  \n",
              "12  100  \n",
              "13  500  \n",
              "14  100  \n",
              "15  500  \n",
              "16  100  \n",
              "17  500  \n",
              "18  100  \n",
              "19  500  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2b5eab7-ecda-4e0d-a368-c9083fb38ccd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>md_tref</th>\n",
              "      <th>md_tdata</th>\n",
              "      <th>max_zemp</th>\n",
              "      <th>zemp</th>\n",
              "      <th>dof</th>\n",
              "      <th>z_chi2</th>\n",
              "      <th>N_0</th>\n",
              "      <th>N0</th>\n",
              "      <th>M</th>\n",
              "      <th>NS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.78</td>\n",
              "      <td>19.17</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.83</td>\n",
              "      <td>10.279797</td>\n",
              "      <td>1.71</td>\n",
              "      <td>20000</td>\n",
              "      <td>1000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.78</td>\n",
              "      <td>107.60</td>\n",
              "      <td>2.13</td>\n",
              "      <td>2.13</td>\n",
              "      <td>10.279797</td>\n",
              "      <td>inf</td>\n",
              "      <td>20000</td>\n",
              "      <td>1000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11.13</td>\n",
              "      <td>24.16</td>\n",
              "      <td>2.29</td>\n",
              "      <td>1.70</td>\n",
              "      <td>11.525841</td>\n",
              "      <td>2.16</td>\n",
              "      <td>20000</td>\n",
              "      <td>5000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.40</td>\n",
              "      <td>42.04</td>\n",
              "      <td>2.39</td>\n",
              "      <td>2.39</td>\n",
              "      <td>12.103694</td>\n",
              "      <td>3.98</td>\n",
              "      <td>20000</td>\n",
              "      <td>5000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11.58</td>\n",
              "      <td>25.80</td>\n",
              "      <td>2.47</td>\n",
              "      <td>1.55</td>\n",
              "      <td>12.580420</td>\n",
              "      <td>2.17</td>\n",
              "      <td>20000</td>\n",
              "      <td>5000</td>\n",
              "      <td>1500</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>11.69</td>\n",
              "      <td>37.63</td>\n",
              "      <td>2.54</td>\n",
              "      <td>2.54</td>\n",
              "      <td>12.786955</td>\n",
              "      <td>3.44</td>\n",
              "      <td>20000</td>\n",
              "      <td>5000</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>12.29</td>\n",
              "      <td>27.46</td>\n",
              "      <td>2.59</td>\n",
              "      <td>1.83</td>\n",
              "      <td>13.192731</td>\n",
              "      <td>2.27</td>\n",
              "      <td>20000</td>\n",
              "      <td>10000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>12.52</td>\n",
              "      <td>30.77</td>\n",
              "      <td>2.64</td>\n",
              "      <td>2.13</td>\n",
              "      <td>13.615087</td>\n",
              "      <td>2.58</td>\n",
              "      <td>20000</td>\n",
              "      <td>10000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>13.33</td>\n",
              "      <td>27.46</td>\n",
              "      <td>2.68</td>\n",
              "      <td>1.66</td>\n",
              "      <td>13.931871</td>\n",
              "      <td>2.14</td>\n",
              "      <td>20000</td>\n",
              "      <td>10000</td>\n",
              "      <td>1500</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13.55</td>\n",
              "      <td>30.37</td>\n",
              "      <td>2.71</td>\n",
              "      <td>1.75</td>\n",
              "      <td>14.149312</td>\n",
              "      <td>2.44</td>\n",
              "      <td>20000</td>\n",
              "      <td>10000</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12.70</td>\n",
              "      <td>28.48</td>\n",
              "      <td>2.74</td>\n",
              "      <td>1.66</td>\n",
              "      <td>13.596791</td>\n",
              "      <td>2.32</td>\n",
              "      <td>50000</td>\n",
              "      <td>1000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12.23</td>\n",
              "      <td>31.05</td>\n",
              "      <td>2.77</td>\n",
              "      <td>2.01</td>\n",
              "      <td>13.230633</td>\n",
              "      <td>2.68</td>\n",
              "      <td>50000</td>\n",
              "      <td>1000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11.92</td>\n",
              "      <td>28.95</td>\n",
              "      <td>2.80</td>\n",
              "      <td>1.87</td>\n",
              "      <td>13.023077</td>\n",
              "      <td>2.47</td>\n",
              "      <td>50000</td>\n",
              "      <td>5000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>11.70</td>\n",
              "      <td>31.26</td>\n",
              "      <td>2.82</td>\n",
              "      <td>2.07</td>\n",
              "      <td>12.804010</td>\n",
              "      <td>2.77</td>\n",
              "      <td>50000</td>\n",
              "      <td>5000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>11.64</td>\n",
              "      <td>28.95</td>\n",
              "      <td>2.84</td>\n",
              "      <td>1.93</td>\n",
              "      <td>12.638322</td>\n",
              "      <td>2.54</td>\n",
              "      <td>50000</td>\n",
              "      <td>5000</td>\n",
              "      <td>1500</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>11.54</td>\n",
              "      <td>31.47</td>\n",
              "      <td>2.87</td>\n",
              "      <td>2.13</td>\n",
              "      <td>12.544567</td>\n",
              "      <td>2.84</td>\n",
              "      <td>50000</td>\n",
              "      <td>5000</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>11.55</td>\n",
              "      <td>28.95</td>\n",
              "      <td>2.88</td>\n",
              "      <td>1.99</td>\n",
              "      <td>12.450808</td>\n",
              "      <td>2.57</td>\n",
              "      <td>50000</td>\n",
              "      <td>10000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>11.66</td>\n",
              "      <td>30.30</td>\n",
              "      <td>2.90</td>\n",
              "      <td>2.01</td>\n",
              "      <td>12.462405</td>\n",
              "      <td>2.72</td>\n",
              "      <td>50000</td>\n",
              "      <td>10000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>11.69</td>\n",
              "      <td>27.30</td>\n",
              "      <td>2.92</td>\n",
              "      <td>1.75</td>\n",
              "      <td>12.387421</td>\n",
              "      <td>2.39</td>\n",
              "      <td>50000</td>\n",
              "      <td>10000</td>\n",
              "      <td>1500</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>11.72</td>\n",
              "      <td>29.42</td>\n",
              "      <td>2.94</td>\n",
              "      <td>2.05</td>\n",
              "      <td>12.420991</td>\n",
              "      <td>2.63</td>\n",
              "      <td>50000</td>\n",
              "      <td>10000</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2b5eab7-ecda-4e0d-a368-c9083fb38ccd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2b5eab7-ecda-4e0d-a368-c9083fb38ccd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2b5eab7-ecda-4e0d-a368-c9083fb38ccd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_df.to_csv(\"GridSearch_Results(Simulated).csv\",index=False)"
      ],
      "metadata": {
        "id": "givimv9h44Oc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_df.sort_values(by='z_chi2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "Zon25wU749If",
        "outputId": "dc6c7a95-b3f8-466c-c265-9fd1d6dd1250"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    md_tref  md_tdata  max_zemp  zemp        dof  z_chi2    N_0     N0     M  \\\n",
              "0      9.78     19.17      1.83  1.83  10.279797    1.71  20000   1000   800   \n",
              "8     13.33     27.46      2.68  1.66  13.931871    2.14  20000  10000  1500   \n",
              "2     11.13     24.16      2.29  1.70  11.525841    2.16  20000   5000   800   \n",
              "4     11.58     25.80      2.47  1.55  12.580420    2.17  20000   5000  1500   \n",
              "6     12.29     27.46      2.59  1.83  13.192731    2.27  20000  10000   800   \n",
              "10    12.70     28.48      2.74  1.66  13.596791    2.32  50000   1000   800   \n",
              "18    11.69     27.30      2.92  1.75  12.387421    2.39  50000  10000  1500   \n",
              "9     13.55     30.37      2.71  1.75  14.149312    2.44  20000  10000  1500   \n",
              "12    11.92     28.95      2.80  1.87  13.023077    2.47  50000   5000   800   \n",
              "14    11.64     28.95      2.84  1.93  12.638322    2.54  50000   5000  1500   \n",
              "16    11.55     28.95      2.88  1.99  12.450808    2.57  50000  10000   800   \n",
              "7     12.52     30.77      2.64  2.13  13.615087    2.58  20000  10000   800   \n",
              "19    11.72     29.42      2.94  2.05  12.420991    2.63  50000  10000  1500   \n",
              "11    12.23     31.05      2.77  2.01  13.230633    2.68  50000   1000   800   \n",
              "17    11.66     30.30      2.90  2.01  12.462405    2.72  50000  10000   800   \n",
              "13    11.70     31.26      2.82  2.07  12.804010    2.77  50000   5000   800   \n",
              "15    11.54     31.47      2.87  2.13  12.544567    2.84  50000   5000  1500   \n",
              "5     11.69     37.63      2.54  2.54  12.786955    3.44  20000   5000  1500   \n",
              "3     11.40     42.04      2.39  2.39  12.103694    3.98  20000   5000   800   \n",
              "1      9.78    107.60      2.13  2.13  10.279797     inf  20000   1000   800   \n",
              "\n",
              "     NS  \n",
              "0   100  \n",
              "8   100  \n",
              "2   100  \n",
              "4   100  \n",
              "6   100  \n",
              "10  100  \n",
              "18  100  \n",
              "9   500  \n",
              "12  100  \n",
              "14  100  \n",
              "16  100  \n",
              "7   500  \n",
              "19  500  \n",
              "11  500  \n",
              "17  500  \n",
              "13  500  \n",
              "15  500  \n",
              "5   500  \n",
              "3   500  \n",
              "1   500  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a951c659-0a3d-4af6-87ae-9cfdd03dbe5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>md_tref</th>\n",
              "      <th>md_tdata</th>\n",
              "      <th>max_zemp</th>\n",
              "      <th>zemp</th>\n",
              "      <th>dof</th>\n",
              "      <th>z_chi2</th>\n",
              "      <th>N_0</th>\n",
              "      <th>N0</th>\n",
              "      <th>M</th>\n",
              "      <th>NS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.78</td>\n",
              "      <td>19.17</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.83</td>\n",
              "      <td>10.279797</td>\n",
              "      <td>1.71</td>\n",
              "      <td>20000</td>\n",
              "      <td>1000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>13.33</td>\n",
              "      <td>27.46</td>\n",
              "      <td>2.68</td>\n",
              "      <td>1.66</td>\n",
              "      <td>13.931871</td>\n",
              "      <td>2.14</td>\n",
              "      <td>20000</td>\n",
              "      <td>10000</td>\n",
              "      <td>1500</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11.13</td>\n",
              "      <td>24.16</td>\n",
              "      <td>2.29</td>\n",
              "      <td>1.70</td>\n",
              "      <td>11.525841</td>\n",
              "      <td>2.16</td>\n",
              "      <td>20000</td>\n",
              "      <td>5000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11.58</td>\n",
              "      <td>25.80</td>\n",
              "      <td>2.47</td>\n",
              "      <td>1.55</td>\n",
              "      <td>12.580420</td>\n",
              "      <td>2.17</td>\n",
              "      <td>20000</td>\n",
              "      <td>5000</td>\n",
              "      <td>1500</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>12.29</td>\n",
              "      <td>27.46</td>\n",
              "      <td>2.59</td>\n",
              "      <td>1.83</td>\n",
              "      <td>13.192731</td>\n",
              "      <td>2.27</td>\n",
              "      <td>20000</td>\n",
              "      <td>10000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12.70</td>\n",
              "      <td>28.48</td>\n",
              "      <td>2.74</td>\n",
              "      <td>1.66</td>\n",
              "      <td>13.596791</td>\n",
              "      <td>2.32</td>\n",
              "      <td>50000</td>\n",
              "      <td>1000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>11.69</td>\n",
              "      <td>27.30</td>\n",
              "      <td>2.92</td>\n",
              "      <td>1.75</td>\n",
              "      <td>12.387421</td>\n",
              "      <td>2.39</td>\n",
              "      <td>50000</td>\n",
              "      <td>10000</td>\n",
              "      <td>1500</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13.55</td>\n",
              "      <td>30.37</td>\n",
              "      <td>2.71</td>\n",
              "      <td>1.75</td>\n",
              "      <td>14.149312</td>\n",
              "      <td>2.44</td>\n",
              "      <td>20000</td>\n",
              "      <td>10000</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11.92</td>\n",
              "      <td>28.95</td>\n",
              "      <td>2.80</td>\n",
              "      <td>1.87</td>\n",
              "      <td>13.023077</td>\n",
              "      <td>2.47</td>\n",
              "      <td>50000</td>\n",
              "      <td>5000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>11.64</td>\n",
              "      <td>28.95</td>\n",
              "      <td>2.84</td>\n",
              "      <td>1.93</td>\n",
              "      <td>12.638322</td>\n",
              "      <td>2.54</td>\n",
              "      <td>50000</td>\n",
              "      <td>5000</td>\n",
              "      <td>1500</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>11.55</td>\n",
              "      <td>28.95</td>\n",
              "      <td>2.88</td>\n",
              "      <td>1.99</td>\n",
              "      <td>12.450808</td>\n",
              "      <td>2.57</td>\n",
              "      <td>50000</td>\n",
              "      <td>10000</td>\n",
              "      <td>800</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>12.52</td>\n",
              "      <td>30.77</td>\n",
              "      <td>2.64</td>\n",
              "      <td>2.13</td>\n",
              "      <td>13.615087</td>\n",
              "      <td>2.58</td>\n",
              "      <td>20000</td>\n",
              "      <td>10000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>11.72</td>\n",
              "      <td>29.42</td>\n",
              "      <td>2.94</td>\n",
              "      <td>2.05</td>\n",
              "      <td>12.420991</td>\n",
              "      <td>2.63</td>\n",
              "      <td>50000</td>\n",
              "      <td>10000</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12.23</td>\n",
              "      <td>31.05</td>\n",
              "      <td>2.77</td>\n",
              "      <td>2.01</td>\n",
              "      <td>13.230633</td>\n",
              "      <td>2.68</td>\n",
              "      <td>50000</td>\n",
              "      <td>1000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>11.66</td>\n",
              "      <td>30.30</td>\n",
              "      <td>2.90</td>\n",
              "      <td>2.01</td>\n",
              "      <td>12.462405</td>\n",
              "      <td>2.72</td>\n",
              "      <td>50000</td>\n",
              "      <td>10000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>11.70</td>\n",
              "      <td>31.26</td>\n",
              "      <td>2.82</td>\n",
              "      <td>2.07</td>\n",
              "      <td>12.804010</td>\n",
              "      <td>2.77</td>\n",
              "      <td>50000</td>\n",
              "      <td>5000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>11.54</td>\n",
              "      <td>31.47</td>\n",
              "      <td>2.87</td>\n",
              "      <td>2.13</td>\n",
              "      <td>12.544567</td>\n",
              "      <td>2.84</td>\n",
              "      <td>50000</td>\n",
              "      <td>5000</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>11.69</td>\n",
              "      <td>37.63</td>\n",
              "      <td>2.54</td>\n",
              "      <td>2.54</td>\n",
              "      <td>12.786955</td>\n",
              "      <td>3.44</td>\n",
              "      <td>20000</td>\n",
              "      <td>5000</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.40</td>\n",
              "      <td>42.04</td>\n",
              "      <td>2.39</td>\n",
              "      <td>2.39</td>\n",
              "      <td>12.103694</td>\n",
              "      <td>3.98</td>\n",
              "      <td>20000</td>\n",
              "      <td>5000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.78</td>\n",
              "      <td>107.60</td>\n",
              "      <td>2.13</td>\n",
              "      <td>2.13</td>\n",
              "      <td>10.279797</td>\n",
              "      <td>inf</td>\n",
              "      <td>20000</td>\n",
              "      <td>1000</td>\n",
              "      <td>800</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a951c659-0a3d-4af6-87ae-9cfdd03dbe5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a951c659-0a3d-4af6-87ae-9cfdd03dbe5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a951c659-0a3d-4af6-87ae-9cfdd03dbe5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_df.groupby(['N_0','N0','M']).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "f-D-l1GT5AiI",
        "outputId": "27941825-9f5e-43fd-d765-983b9b508f9e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  md_tref  md_tdata  max_zemp   zemp        dof  z_chi2     NS\n",
              "N_0   N0    M                                                                 \n",
              "20000 1000  800     9.780    63.385     1.980  1.980  10.279797     inf  300.0\n",
              "      5000  800    11.265    33.100     2.340  2.045  11.814768   3.070  300.0\n",
              "            1500   11.635    31.715     2.505  2.045  12.683688   2.805  300.0\n",
              "      10000 800    12.405    29.115     2.615  1.980  13.403909   2.425  300.0\n",
              "            1500   13.440    28.915     2.695  1.705  14.040591   2.290  300.0\n",
              "50000 1000  800    12.465    29.765     2.755  1.835  13.413712   2.500  300.0\n",
              "      5000  800    11.810    30.105     2.810  1.970  12.913543   2.620  300.0\n",
              "            1500   11.590    30.210     2.855  2.030  12.591444   2.690  300.0\n",
              "      10000 800    11.605    29.625     2.890  2.000  12.456607   2.645  300.0\n",
              "            1500   11.705    28.360     2.930  1.900  12.404206   2.510  300.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d21a3b8-27ec-4ea3-b6db-e308efc70f8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>md_tref</th>\n",
              "      <th>md_tdata</th>\n",
              "      <th>max_zemp</th>\n",
              "      <th>zemp</th>\n",
              "      <th>dof</th>\n",
              "      <th>z_chi2</th>\n",
              "      <th>NS</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>N_0</th>\n",
              "      <th>N0</th>\n",
              "      <th>M</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">20000</th>\n",
              "      <th>1000</th>\n",
              "      <th>800</th>\n",
              "      <td>9.780</td>\n",
              "      <td>63.385</td>\n",
              "      <td>1.980</td>\n",
              "      <td>1.980</td>\n",
              "      <td>10.279797</td>\n",
              "      <td>inf</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">5000</th>\n",
              "      <th>800</th>\n",
              "      <td>11.265</td>\n",
              "      <td>33.100</td>\n",
              "      <td>2.340</td>\n",
              "      <td>2.045</td>\n",
              "      <td>11.814768</td>\n",
              "      <td>3.070</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>11.635</td>\n",
              "      <td>31.715</td>\n",
              "      <td>2.505</td>\n",
              "      <td>2.045</td>\n",
              "      <td>12.683688</td>\n",
              "      <td>2.805</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
              "      <th>800</th>\n",
              "      <td>12.405</td>\n",
              "      <td>29.115</td>\n",
              "      <td>2.615</td>\n",
              "      <td>1.980</td>\n",
              "      <td>13.403909</td>\n",
              "      <td>2.425</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>13.440</td>\n",
              "      <td>28.915</td>\n",
              "      <td>2.695</td>\n",
              "      <td>1.705</td>\n",
              "      <td>14.040591</td>\n",
              "      <td>2.290</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">50000</th>\n",
              "      <th>1000</th>\n",
              "      <th>800</th>\n",
              "      <td>12.465</td>\n",
              "      <td>29.765</td>\n",
              "      <td>2.755</td>\n",
              "      <td>1.835</td>\n",
              "      <td>13.413712</td>\n",
              "      <td>2.500</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">5000</th>\n",
              "      <th>800</th>\n",
              "      <td>11.810</td>\n",
              "      <td>30.105</td>\n",
              "      <td>2.810</td>\n",
              "      <td>1.970</td>\n",
              "      <td>12.913543</td>\n",
              "      <td>2.620</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>11.590</td>\n",
              "      <td>30.210</td>\n",
              "      <td>2.855</td>\n",
              "      <td>2.030</td>\n",
              "      <td>12.591444</td>\n",
              "      <td>2.690</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">10000</th>\n",
              "      <th>800</th>\n",
              "      <td>11.605</td>\n",
              "      <td>29.625</td>\n",
              "      <td>2.890</td>\n",
              "      <td>2.000</td>\n",
              "      <td>12.456607</td>\n",
              "      <td>2.645</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>11.705</td>\n",
              "      <td>28.360</td>\n",
              "      <td>2.930</td>\n",
              "      <td>1.900</td>\n",
              "      <td>12.404206</td>\n",
              "      <td>2.510</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d21a3b8-27ec-4ea3-b6db-e308efc70f8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6d21a3b8-27ec-4ea3-b6db-e308efc70f8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6d21a3b8-27ec-4ea3-b6db-e308efc70f8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Make sure we use the subsample in our correlation\n",
        "\n",
        "f, ax1 = plt.subplots(1, 1, figsize=(24,20))\n",
        "\n",
        "# Entire DataFrame\n",
        "corr = res_df.corr()\n",
        "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
        "ax1.set_title(\"Correlation Matrix (Grid Search_SimulatedDataset)\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R7_vctP6jeWW",
        "outputId": "b7ce4035-9fd7-401a-af42-1dca92726043"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1728x1440 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPH9SHoN9k9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}